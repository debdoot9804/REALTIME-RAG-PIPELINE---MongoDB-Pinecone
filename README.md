# REALTIME-RAG-PIPELINE---MongoDB & Pinecone





Gemma 2 LLM: Takes retrieved docs + user query and generates a final answer

âœ…MongoDB: Stores raw documents/data.

âœ…MongoDB Stream Listener: Watches for changes in the database.

âœ…Pinecone: Stores vector embeddings and handles semantic search

âœ…All-MiniLM-L6-v2 for lightweight, fast embedding generation

âœ…Retriever: Finds top-k relevant documents from Pinecone.

âœ…Groqâ€™s Gemma-2 9B:  LLM Takes retrieved docs + user query and generates a final answer

ğŸ“¦ The system auto-embeds documents on insert via a MongoDB change stream listener, upserts them to Pinecone, and retrieves relevant context on query to generate answers via Groq â€” all inside Google Colab notebooks.

ğŸ§ª Tech Stack:

MongoDB (Change Streams + Metadata)

Pinecone (Vector DB)

Hugging Face Sentence Transformers (all-MiniLM-L6-v2)

Groq API (Gemma-2 9B)

Google Colab + Python

ğŸ“ Architecture:

![Editor _ Mermaid Chart-2025-04-06-073618](https://github.com/user-attachments/assets/675220a2-26e6-4be9-9673-e934b0c4d8a5)



