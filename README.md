# REALTIME-RAG-PIPELINE---MongoDB & Pinecone

A modular, real-time RAG system that connects:

âœ… MongoDB (as a metadata store) to store and track documents
âœ… Pinecone (as a vector store) to handle semantic retrieval
âœ… All-MiniLM-L6-v2 for lightweight, fast embedding generation
âœ… Groqâ€™s Gemma-2 9B LLM for fast and accurate responses

ğŸ“¦ The system auto-embeds documents on insert via a MongoDB change stream listener, upserts them to Pinecone, and retrieves relevant context on query to generate answers via Groq â€” all inside Google Colab notebooks.

ğŸ§ª Tech Stack:

MongoDB (Change Streams + Metadata)

Pinecone (Vector DB)

Hugging Face Sentence Transformers (all-MiniLM-L6-v2)

Groq API (Gemma-2 9B)

Google Colab + Python

ğŸ“ Diagram Flow Breakdown

ğŸ” Ingestion Path


[ MongoDB ] â”€â”€â–º [ MongoDB Change Stream Listener ] â”€â”€â–º [ MiniLM Embeddings ]
        â”‚                                                      â”‚
        â–¼                                                      â–¼
[ Metadata Store ]                                     [ Pinecone Vector Store ]

----------------------------------------------------------------------------------------------------------------------

ğŸ” Retrieval + Generation Path

[ User Query ] â”€â”€â–º [ MiniLM (Embedding) ] â”€â”€â–º [ Pinecone Search (Top-k) ] â”€â”€â–º
        â–¼
[ Retrieved Context ] â”€â”€â–º [ Groq API (Gemma-2 9B) ] â”€â”€â–º [ Final Response ]

----------------------------------------------------------------------------------------------------------------------

